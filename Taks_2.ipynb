{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDMtwFFJg0vs94VoW5oWo8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiuChen-5749342/Generative-AI-and-AI-Applications/blob/main/Taks_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploring Autograd with a Complex Function**\n",
        "\n",
        "This exercise tests your understanding of PyTorch's automatic differentiation engine, Autograd. You will manually define a function with multiple variables, ask PyTorch to compute the gradients, and then verify the results.\n",
        "\n",
        "Your Goal: Calculate the partial derivatives of the following function with respect to w, x, and b.\n",
        "\n",
        "Let the function be:\n",
        "z=σ(w.x2)+1b3\n",
        "\n",
        "where σ is the sigmoid function, which in PyTorch is torch.sigmoid().\n",
        "\n",
        "Given Initial Values:\n",
        "w=2.0\n",
        "x=4.0\n",
        "b=1.5"
      ],
      "metadata": {
        "id": "6xBn0kKre08F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task:**\n",
        "\n",
        "Create three tensors for w, x, and b with the given initial values. Make sure PyTorch tracks their gradients.\n",
        "\n",
        "Write the Python code to compute z using these tensors.\n",
        "\n",
        "Call the .backward() method to compute the gradients.\n",
        "\n",
        "Print the computed gradients for w, x, and b.\n",
        "\n",
        "**Hint:** Remember to set requires_grad=True when you create the tensors. The gradient of a tensor t is stored in t.grad after you call .backward() on the final output.\n"
      ],
      "metadata": {
        "id": "NhG2HBoHfMQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Import and Initialization**\n",
        "\n",
        "Import the torch library.\n",
        "\n",
        "Define the three scalar tensors w, x, and b as floats with their respective initial values (2.0, 4.0, 1.5).\n",
        "\n",
        "Explicitly set requires_grad=True for all three so Autograd tracks them."
      ],
      "metadata": {
        "id": "RhJ32SMYiFUF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJB2fMiDexds",
        "outputId": "ab12c4c2-4f6e-4c42-9ecd-0a12eeb7e007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Tensors:\n",
            "w: 2.0\n",
            "x: 4.0\n",
            "b: 1.5\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Define the tensors with their initial values\n",
        "# requires_grad=True is the magic flag that enables gradient tracking\n",
        "w = torch.tensor(2.0, requires_grad=True)\n",
        "x = torch.tensor(4.0, requires_grad=True)\n",
        "b = torch.tensor(1.5, requires_grad=True)\n",
        "\n",
        "# Let's print them to verify their state\n",
        "print(\"Initialized Tensors:\")\n",
        "print(f\"w: {w}\")\n",
        "print(f\"x: {x}\")\n",
        "print(f\"b: {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: The Forward Pass**\n",
        "\n",
        "Write the PyTorch equivalent of the mathematical formula to compute z.\n",
        "\n",
        "Use torch.sigmoid() for σ, and standard Python/PyTorch arithmetic operators."
      ],
      "metadata": {
        "id": "dxIUIiDfj-NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate z = sigmoid(w * (x ** 2)) + (1 / (b ** 3))\n",
        "z = torch.sigmoid(w * (x ** 2)) + (1 / (b ** 3))\n",
        "\n",
        "# Let's look at the resulting tensor z\n",
        "print(\"\\nForward Pass Result:\")\n",
        "print(f\"z: {z}\")\n",
        "print(f\"z's grad_fn: {z.grad_fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUYTJLmpj-1b",
        "outputId": "d387787a-da61-44be-ebe7-9f04c9c68cc9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forward Pass Result:\n",
            "z: 1.2962963581085205\n",
            "z's grad_fn: <AddBackward0 object at 0x7c48783db160>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: The Backward Pass**\n",
        "\n",
        "Execute z.backward(). This single line tells PyTorch to compute all the partial derivatives based on the computational graph that was just built."
      ],
      "metadata": {
        "id": "KN8_rA2alJFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the gradients of z with respect to all leaf tensors\n",
        "z.backward()\n",
        "\n",
        "print(\"\\nBackward pass completed. Gradients have been calculated and stored.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phUgBxOGjgLj",
        "outputId": "ab596eb6-e388-414e-eb5f-a141061cf6d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Backward pass completed. Gradients have been calculated and stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Output and Verification**\n",
        "\n",
        "Print out the computed gradients accessing w.grad, x.grad, and b.grad.\n",
        "\n",
        "(Optional but recommended) Include a brief comment on the mathematical expectation to verify PyTorch's output makes sense."
      ],
      "metadata": {
        "id": "Sbjdn_kIlOir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output and Verification\n",
        "print(\"\\nComputed Gradients:\")\n",
        "print(f\"dz/dw (w.grad): {w.grad}\")\n",
        "print(f\"dz/dx (x.grad): {x.grad}\")\n",
        "print(f\"dz/db (b.grad): {b.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQfB0rkflIJ2",
        "outputId": "2165d707-be49-4482-faff-20fbe0ec613f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computed Gradients:\n",
            "dz/dw (w.grad): 0.0\n",
            "dz/dx (x.grad): 0.0\n",
            "dz/db (b.grad): -0.5925925970077515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hhrxeQRTljAU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}